---
title: "Exercise 2"
author: "Damian Ke & Kyriakos Papadopoulos"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(caret)
library(glmnet)
```
## Assignment 1. Explicit regularization

**Question 1**: Assume that Fat can be modeled as a linear regression in which
absorbance characteristics (Channels) are used as features. Report the underlying
probabilistic model, fit the linear regression to the training data and estimate
the training and test errors. Comment on the quality of fit and prediction and
therefore on the quality of model.


**Answer 1:**
```{r, echo=FALSE}
# Splitting the data
data = read.csv("tecator.csv")

# Removing the sample column
data = data[-1]
# scaler = preProcess(data)
# data = predict(scaler, data)
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]

fit = lm(Fat ~ . - Protein - Moisture , data = train)

results_training = predict(fit, train)
mse_training =  mean((results_training - train$Fat) ^ 2)

results_test = predict(fit, test)
mse_test = mean((results_test - test$Fat) ^ 2)
```

We chose to use **MSE** as our error function because we have a regression problem.

```{r, echo = FALSE}
cat("The mse for training data is: ", mse_training, "\n")
cat("The mse for test data is: ", mse_test)
```

We can clearly see that there is a big difference between training and test errors.
The training error is much smaller compared to the test error. That shows that
our model is very complicated and it overfits.

The probabilistic model for linear regression is given by the formula:
$$y|x~N(\theta^{T}x, \sigma^{2})$$

 which is equal if we write it like as equation:
 
 $$y = \theta_0 + \theta+1 x_1 + ... \theta_px_p + \epsilon$$

The $\epsilon$ represents the noise term and accounts for random errors in the data
not captured by the model. The noise is assumed to have mean zero and to be independent 
of x. The zero-mean assumption
is nonrestrictive, since any (constant) non-zero mean can be incorporated in the
offset term $\theta_0$

**Question 2: ** Assume now that Fat can be modeled as a LASSO regression in which all
Channels are used as features. Report the cost function that should be
optimized in this scenario.

**Answer 2: ** The cost function that we are gonna use for LASSO regression according
to the slides:

$$\frac{1}{n} \sum_{i = 1}^{n} (y_i - \theta0 - \theta_1 x_1j - ... - \theta_p x_pj) ^ 2 + \lambda \sum_{j=1}^{p} |\theta_i|$$

Where $p = 100$ in our case.

**Question 3: ** Fit the LASSO regression model to the training data. Present a 
plot illustrating how the regression coefficients depend on the log of penalty
factor (log $\lambda$) and interpret this plot. What value of the penalty factor 
can be chosen if we want to select a model with only three features?

**Answer 3: **

```{r, echo=FALSE, fig.align='center'}
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]

modell = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian")
plot(modell, xvar="lambda", label=TRUE)

# values_of_lambda = c()
# for(i in 1:length(modell$df)){
#   if(modell$df[i] == 3){
#     values_of_lambda <- append(values_of_lambda, modell$lambda[i])
#   }
# }
# 
# # The values of lambda that we have df =  3
# from = min(values_of_lambda)
# to = max(values_of_lambda)

```

```{r, echo=FALSE}
lambdas = which(modell$df == 3)
cat("The values of lambda that we get 3 features are: ", modell$lambda[lambdas])
```
We can also do it visually by going to the above plot and find the value of lambda that only 3 lines are != 0. 

**Question 4: ** Repeat step 3 but fit Ridge instead of the LASSO regression and 
compare the plots from steps 3 and 4. Conclusions?

**Answer 4: **
```{r, echo=FALSE, fig.align='center'}
modelr = glmnet(as.matrix(covariates), response, alpha = 0, family="gaussian")
plot(modelr, xvar="lambda", label=TRUE)
```

We can see that for the same value of lambda LASSO gets rid of much more coefficients
compared to Ridge. That's because LASSO takes the magnitude of the coefficients
and ridge takes the square.

**Question 5: **Use cross-validation with default number of folds to compute the optimal
LASSO model. Present a plot showing the dependence of the CV score on log $\lambda$
and comment how the CV score changes with log $\lambda$. Report the optimal $\lambda$ and how many variables were chosen in this model. Does the information
displayed in the plot suggests that the optimal $\lambda$ value results in a statistically
significantly better prediction than log $\lambda$ = -4? Finally, create a scatter plot of the original test versus predicted test values for the model corresponding
to optimal lambda and comment whether the model predictions are good

**Answer 5:**

```{r, echo=FALSE, fig.align='center'}
modelcv=cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
optimal_lambda = modelcv$lambda.min
plot(modelcv)
cv_scores = modelcv$cvm
lambdas = modelcv$lambda
optimal_lambda = modelcv$lambda.min

```

```{r, include=FALSE, fig.align='center'}
pred = predict(modelcv, as.matrix(test[, 1:100]))
optimal_mse = (mean(pred - test$Fat)^2)

df = data.frame(pred, test$Fat)

p <- ggplot(data = df, aes(x=test.Fat, y=pred, color = "Predictions")) + 
  geom_point() + geom_line(data = df, aes(x=test.Fat, y = test.Fat, color = "Perfect Regression Line")) + labs(x = "Real Values", y = "Predictions", color = "Legend")

# Question: How we are sure for the number of features? from optimal_model$beta
# or from the diagramm?
```
```{r, echo=FALSE}
```

We can see that the cv score is stable until the optimal lambda. After the optimal 
lambda the cv score starts to increase. 

We can see that the confidence intervals are equal so for the optimal $\lambda$ it's not  significantly better in predictions compared to $\lambda = log(-4)$. The optimal $\lambda$ is where is the first dotted line in the plot.

```{r, echo=FALSE}
#optimal_model$beta
```

We can see that the model with the optimal $\lambda$ uses in total 9 features. 

```{r, echo=FALSE, fig.align='center'}
n_of_coef = modelcv$nzero[which(modelcv$lambda.min == modelcv$lambda) ]
cat("The optimal lambda is ", modelcv$lambda.min, "and it has ", n_of_coef, " features") 
```
The red line represents a perfect model and the blue dots are our predictions. We
can see that our model is not perfect but it's predictions are close to the real
values. 