return(loglikelihood)
}
ridgeOpt <- function(lambda, data){
# We initialize every value at 1. We take the number of columns of data as we
#know that we don't need intercept but we need sigma parameter so it's number
#of parameters - 1(for intercept) + 1(for sigma) = number of parameters
opt <- optim(rep(1, ncol(data) ), fn=ridge, data=data, lambda=lambda, method="BFGS")
return(opt)
}
df <- function(lambda, x){
x<- as.matrix(x)
# P hat matrix
P_hat <- x %*% solve((t(x) %*% x) + diag(lambda, ncol(x))) %*% t(x)
out <- sum(diag(P_hat))
return(out)
}
trainS_hat = trainS["motor_UPDRS"]
#trainS = subset(trainS, select = -c(motor_UPDRS))
testS_hat = testS["motor_UPDRS"]
#testS = subset(testS, select = -c(motor_UPDRS))
model1 = ridgeOpt(1, trainS)
model2 = ridgeOpt(100, trainS)
model3 = ridgeOpt(1000, trainS)
opt_predict <- function(x, model){
# sigma doesn't take part in calculations
out <- sum((x) * model$par[-1])
return(out)
}
train_output1 <- c()
train_output2 <- c()
train_output3 <- c()
# Removing y column as we need only x
trainS = subset(trainS, select = -c(motor_UPDRS))
testS = subset(testS, select = -c(motor_UPDRS))
for(i in 1:nrow(trainS)){
train_output1[i] <- opt_predict(trainS[i, ], model1)
train_output2[i] <- opt_predict(trainS[i, ], model2)
train_output3[i] <- opt_predict(trainS[i, ], model3)
}
test_output1 <- c()
test_output2 <- c()
test_output3 <- c()
for(i in 1:nrow(testS)){
test_output1[i] <- opt_predict(testS[i, ], model1)
test_output2[i] <- opt_predict(testS[i, ], model2)
test_output3[i] <- opt_predict(testS[i, ], model3)
}
ms_train1 <- (1/nrow(trainS_hat)) * (sum(train_output1 - trainS_hat)) ^ 2
ms_train2 <- (1/nrow(trainS_hat)) * (sum(train_output2 - trainS_hat)) ^ 2
ms_train3 <- (1/nrow(trainS_hat)) * (sum(train_output3 - trainS_hat)) ^ 2
ms_test1 <- (1/nrow(testS_hat)) * (sum(test_output1 - testS_hat))^ 2
ms_test2 <- (1/nrow(testS_hat)) * (sum(test_output2 - testS_hat)) ^ 2
ms_test3 <- (1/nrow(testS_hat)) * (sum(test_output3 - testS_hat)) ^ 2
ms_train1
ms_train2
ms_train3
ms_test1
ms_test2
ms_test3
paste0("for lambda = 1 in training data, the df are ",df(1, trainS))
paste0("for lambda = 100 in training data, the df are ",df(100, trainS))
paste0("for lambda = 1000 in training data, the df are ",df(1000, trainS))
paste0("for lambda = 1 in test data, the df are ",df(1, testS))
paste0("for lambda = 100 in test data, the df are ",df(100, testS))
paste0("for lambda = 1000 in test data, the df are ",df(1000, testS))
trainS_hat = trainS["motor_UPDRS"]
data = read.csv("parkinsons.csv")
data = subset(data, select = -c(age, sex, test_time, subject., total_UPDRS))
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.6))
trainS = data[id,]
testS = data[-id,]
# For intercept = 0
fit = lm(motor_UPDRS ~ 0 + ., data = trainS)
# Calculating the MSE on training
#model_summ <-summary(fit)
#mse_training1 = mean(model_summ$residuals^2) # That's a second way for training MSE
results = predict(fit, trainS)
n = length(results)
mse_training = (1/n) * sum((results - trainS$motor_UPDRS)^2)
print(paste0("The training MSE is: ", mse_training))
# Calculating the MSE on test
results = predict(fit, testS)
n = length(results)
mse_test = (1/n) * sum((results - testS$motor_UPDRS)^2)
print(paste0("The test MSE is: ", mse_test))
# Finding the most significant variables
summary(fit)
loglikelihood <- function(params, data){
weights <- params[2:ncol(data)]
sigma <- params[1]
x <- data[, 2:ncol(data)]
y <- data[,1]
n <- nrow(x)
loglikelihood <- -n / 2 * log(2 * pi * sigma ^ 2) - 1 / (2 * sigma ^ 2) *
sum((y - as.matrix(x) %*% as.numeric(weights)) ^ 2)
return(loglikelihood)
}
ridge <- function(params, data, lambda){
w <- params[2:length(params)]
loglikelihood <- -loglikelihood(params, data) + lambda*sum(w^2)
return(loglikelihood)
}
ridgeOpt <- function(lambda, data){
# We initialize every value at 1. We take the number of columns of data as we
#know that we don't need intercept but we need sigma parameter so it's number
#of parameters - 1(for intercept) + 1(for sigma) = number of parameters
opt <- optim(rep(1, ncol(data) ), fn=ridge, data=data, lambda=lambda, method="BFGS")
return(opt)
}
df <- function(lambda, x){
x<- as.matrix(x)
# P hat matrix
hat_matrix <- x %*% solve((t(x) %*% x) + diag(lambda, ncol(x))) %*% t(x)
out <- sum(diag(hat_matrix))
return(out)
}
trainS_hat = trainS["motor_UPDRS"]
#trainS = subset(trainS, select = -c(motor_UPDRS))
testS_hat = testS["motor_UPDRS"]
#testS = subset(testS, select = -c(motor_UPDRS))
model1 = ridgeOpt(1, trainS)
model2 = ridgeOpt(100, trainS)
model3 = ridgeOpt(1000, trainS)
opt_predict <- function(x, model){
# sigma doesn't take part in calculations
out <- sum((x) %*% model$par[-1])
return(out)
}
train_output1 <- c()
train_output2 <- c()
train_output3 <- c()
# Removing y column as we need only x
trainS = subset(trainS, select = -c(motor_UPDRS))
testS = subset(testS, select = -c(motor_UPDRS))
for(i in 1:nrow(trainS)){
train_output1[i] <- opt_predict(trainS[i, ], model1)
train_output2[i] <- opt_predict(trainS[i, ], model2)
train_output3[i] <- opt_predict(trainS[i, ], model3)
}
trainS_hat = trainS["motor_UPDRS"]
df(0, testS)
prob
library(ggplot2)
diabetes = read.csv("pima-indians-diabetes.csv", header=FALSE)
p <- ggplot(diabetes, aes(x=V8,y=V2,color=as.factor(V9)))+
geom_point() +
labs(color = "Diabetes")+
xlab("Age")+
ylab("Plasma glucose concentration")+
ggtitle("Plasma glucose concentration on Age")
p
m1=glm(V9~V2+V8, data=diabetes, family = "binomial")
summary(m1)$coef
missclass=function(X){
n=sum(X)
a = sum(diag(X))/n
return(1-a)
}
Prob=predict(m1, type="response")
Pred=ifelse(Prob>0.5, "Yes", "No")
confusion_matrix_train = table(diabetes$V9, Pred)
missclass(confusion_matrix_train)
diabetes["Predicted"] = Pred
p <- ggplot(diabetes, aes(x=V8,y=V2,color=Predicted))+
geom_point() +
labs(color = "Predicted diabetes")+
xlab("Age")+
ylab("Plasma glucose concentration")+
ggtitle("Plasma glucose concentration on Age")
p
Prob
confusion_matrix_train
exp(1)
exp(0)
exp(1)
abs(-4)
abs(4)
?sign
sign(1:10)
sign(-1,1)
sign(c(-1,1))
sign(c(-1,100))
de <- function(a, mi, n){
x <- 1:n
y <- (a/2) * exp(-a * abs(x - mi))
return(y)
}
de(6,2,100)
de(6,100,100)
de(1,1,100)
invcdf <- function(m, a, x){
y = (log(2*x)/a) + m
return(x)
}
de(10, 1, runif(1000))
de <- function(a, mi, x){
y <- (a/2) * exp(-a * abs(x - mi))
return(y)
}
de(10, 1, runif(1000))
y1 = de(10, 1, runif(1000))
plot(y1)
plot(y1, "l")
plot(y1, "l")
plot(sort(y1), "l")
plot(sort(y1))
laplace_density = function(x, mu, alpha) {
result = (alpha/2)*exp(-alpha*abs(x-mu))
return(result)
}
n = 10000
unif_sample = runif(n)
sample = laplace_inv_cdf(unif_sample, 10, 1)
n = 10000
unif_sample = runif(n)
sample = laplace_density(unif_sample, 10, 1)
plot(sample)
set.seed(12345)
y1 = de(10, 1, runif(1000))
plot(sort(y1))
set.seed(12345)
n = 10000
unif_sample = runif(n)
sample = laplace_density(unif_sample, 10, 1)
plot(sort(sample))
laplace_density(1,1,1)
df(1,1,1)
de(1,1,1)
?sign
invcdf <- function(m, a, x){
y = (log(2*x)/a) + m
return(y)
}
invcdf <- function(m, a, x){
y = (log(2*x)/a) + m
return(y)
}
set.seed(12345)
x <- runif(1000)
y1 = de(10, 1, x)
y2 = invcdf(10, 11, x)
df = data.frame(y1, y2)
length(y1)
length(y2)
df = data.frame(y1, y2, i=1:length(y1))
p <- ggplot(df, aes(x=i, y=y1))
library(ggplot2)
p <- ggplot(df, aes(x=i, y=y1))
p
p <- ggplot(df, aes(x=i, y=y1)) + geom_histogram()
p
df
u<-Reduce(function(s,r) {s*2+r}, v)
v<-c(1,0,1)
u<-Reduce(function(s,r) {s*2+r}, v)
u
v<-c(1,0,5)
u<-Reduce(function(s,r) {s*2+r}, v)
u
v<-c(1,10)
v
u
u<-Reduce(function(s,r) {s*2+r}, v)
u
v<-c(1,10000)
u<-Reduce(function(s,r) {s*2+r}, v)
u
library(caret)
library(glmnet)
# Splitting the data
data = read.csv("tecator.csv")
setwd("C:/Users/kyria/OneDrive/Υπολογιστής/Statistics And Machine Learning/Machine Learning/Assigment 1/Machine-Learning-Lab/LAB2")
library(caret)
library(glmnet)
# Splitting the data
data = read.csv("tecator.csv")
# Removing the sample column
data = data[-1]
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
## 1.
fit = lm(Fat ~ . - Protein - Moisture , data = train)
summary(fit)
results_training = predict(fit, train)
mse_training = (1/n) * sum((results_training - train$Fat) ^ 2)
results_test = predict(fit, test)
mse_test = (1/n) * sum((results_test - test$Fat) ^ 2)
# Question 1: Can I use MSE as error function?
# Question 2: Lasso is widely used when p>>n, why to use it here?
# MSE of training is much smaller compared to the MSE of training. That's because
# of overfitting, so our model is very complicated and it can generalize well.
# Probably it needs less features
## 2.
# Question 1: Cost function at slide 20 2d?
# Question 2: According to LASSO we should scale our data, but exercise doesn't mention
# something like that
## 3.
# Question 1: Should we do it like that or use the optim function?
# Does it mean to show the degrees of freedom depending on lambda?
# The first column in the number of samples
scaler = preProcess(train)
trainS = predict(scaler, train)
testS = predict(scaler, test)
covariates = trainS[, !names(trainS) %in% c("Fat")]
response = trainS[, names(trainS) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lamda = 0)
model0$lambda
?glmnet
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = 0)
summary(model0)
model0
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = 1)
model0
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian")
plot(model0, xvar="lambda", label=TRUE)
library(caret)
library(glmnet)
# Splitting the data
data = read.csv("tecator.csv")
# Removing the sample column
data = data[-1]
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
# The first column in the number of samples
scaler = preProcess(train)
trainS = predict(scaler, train)
testS = predict(scaler, test)
covariates = trainS[, !names(trainS) %in% c("Fat", "Protein", "Moisture")]
response = trainS[, names(trainS) %in% c("Fat")
# Splitting the data
data = read.csv("tecator.csv")
# Removing the sample column
data = data[-1]
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
# Does it mean to show the degrees of freedom depending on lambda?
# The first column in the number of samples
scaler = preProcess(train)
trainS = predict(scaler, train)
testS = predict(scaler, test)
covariates = trainS[, !names(trainS) %in% c("Fat", "Protein", "Moisture")]
response = trainS[, names(trainS) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian")
plot(model0, xvar="lambda", label=TRUE)
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian")
plot(model0, xvar="lambda", label=TRUE)
plot(model0, xvar="lambda", label=TRUE)
plot(model0, xvar="lambda", label=TRUE)
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
?glmnet
library(caret)
library(glmnet)
# Splitting the data
data = read.csv("tecator.csv")
# Removing the sample column
data = data[-1]
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
par(mar=c(1,1,1,1))
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
plot(model0, xvar="lambda", label=TRUE)
scaler = preProcess(train)
trainS = predict(scaler, train)
testS = predict(scaler, test)
covariates = trainS[, !names(trainS) %in% c("Fat", "Protein", "Moisture")]
response = trainS[, names(trainS) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
library(caret)
library(glmnet)
# Splitting the data
data = read.csv("tecator.csv")
# Removing the sample column
data = data[-1]
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
test = data[-id,]
## 1.
fit = lm(Fat ~ . - Protein - Moisture , data = train)
summary(fit)
results_training = predict(fit, train)
mse_training = (1/n) * sum((results_training - train$Fat) ^ 2)
results_test = predict(fit, test)
mse_test = (1/n) * sum((results_test - test$Fat) ^ 2)
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
model0 = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(model0, xvar="lambda", label=TRUE)
model0$lambda
model0$beta
options(max.print= *, width = *)
options(max.print= *, width = *)
model0$beta
model0$beta[1,]
model0$df
length(model0$df)
length(model0$lambda)
lmodel0$lambda
model0$lambda
for(df in model0$df){
print(df)
}
model$df
model0$df
values_of_lambda = c()
for(i in 1:length(model0$df)){
if(model0$df[i] == 3){
values_of_lambda <- append(values_of_lambda, model0$lambda[i])
}
}
values_of_lambda
min(values_of_lambda)
from = min(values_of_lambda)
to = max(values_of_lambda)
from
to
model$df
model0$df
modelr = glmnet(as.matrix(covariates), response, alpha = 0, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(modelr, xvar="lambda", label=TRUE)
modelcv=cv.glmnet(as.matrix(covariates), response, alpha=1, family="gaussian")
modelcv$lambda.min
plot(modelcv)
par(c(1,1,1,1))
par(mar=c(1,1,1,1))
plot(modelcv)
coef(modelcv, s="lambda.min")
modelcv$lambda.min
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
modell = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(modell, xvar="lambda", label=TRUE
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
covariates = train[, !names(train) %in% c("Fat", "Protein", "Moisture")]
response = train[, names(train) %in% c("Fat")]
modell = glmnet(as.matrix(covariates), response, alpha = 1, family="gaussian",
lambda = seq(0, 1, 0.001))
plot(modell, xvar="lambda", label=TRUE)
modelcv$lambda.1se
modelcv$lambda
modelcv$lambda.min
modelcv$cvm
length(modelcv$cvm)
length(modelcv$lambda)
df = data.frame(cv_scores, lambdas)
cv_scores = modelcv$cvm
lambdas = modelcv$lambda
df = data.frame(cv_scores, lambdas)
df
p = ggplot(data = df, aes(x=lambdas, y=cv_scores)) +
geom_line()
p
p = ggplot(data = df, aes(x=log(lambdas, base = 10), y=cv_scores)) +
geom_line()
p
lambdas
cv_scores
plot(modelcv)
optimal_lambda = modelcv$lambda.min
optimal_lambda
which(optimal_lambda == lambdas)
?cv.glmnet
